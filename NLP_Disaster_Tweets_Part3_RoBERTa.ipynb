{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"duration":2852.740596,"end_time":"2020-11-14T05:05:37.439963","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-11-14T04:18:04.699367","version":"2.1.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"099bf78053064c5b9df4ddcff9a2cac5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"14610be3ca1c40d193777716a0fd284c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"2a1cd0b84833438ba969b6c24afacef7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a46813e211348ba8d23ea5ef829e614":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_9aaf7ad682724241ba82caf4c60524ef","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_099bf78053064c5b9df4ddcff9a2cac5","value":898823}},"3897e5ee3e8d4022864b16db82959f5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"392bad764ac64553bcdfbd089c8d84f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce18b1f07d5d46f0aef808d27de42937","IPY_MODEL_57f363f103de488da83b52f09421849a"],"layout":"IPY_MODEL_dbb2ad99fc214361850496ac30ce73aa"}},"46c524f2587245ae8839df232eecf0cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac30452a79d048a2a8f4a825e9202cf8","IPY_MODEL_9d190361af9e4cc0bfa7465e959f7815"],"layout":"IPY_MODEL_c1ee69ff85e94581ad8db4389ef30f0f"}},"57f363f103de488da83b52f09421849a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a84ee9104d7040cabca70d94a22b73fa","placeholder":"‚Äã","style":"IPY_MODEL_742619243e3648f898538d8706d5a46a","value":" 501M/501M [00:18&lt;00:00, 26.6MB/s]"}},"5ebcbdd25660430c994818e4054f92e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a46813e211348ba8d23ea5ef829e614","IPY_MODEL_edfa9e55c5a14d5d8d226386b127c581"],"layout":"IPY_MODEL_afde83aea74547f097b05332c1498592"}},"6e0b90816d5f4fe88cb9fbee376d4f13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73be276a24c04fb0a964b09dc3691f3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"742619243e3648f898538d8706d5a46a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"977e76a538a44f909b9d53684685f97f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aaf7ad682724241ba82caf4c60524ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b87f0b3b57b415ebfc71337c13b5e4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab3ce48dc29d4d4bb838dd2d6f3d559f","IPY_MODEL_b5d8fa159b664b32b62861aac6de1cd5"],"layout":"IPY_MODEL_d9080b485f7f41f1baba6d8a435ee099"}},"9d190361af9e4cc0bfa7465e959f7815":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73be276a24c04fb0a964b09dc3691f3e","placeholder":"‚Äã","style":"IPY_MODEL_b4d3ca74fdd04380ab975d906f94b553","value":" 481/481 [00:05&lt;00:00, 94.8B/s]"}},"a84ee9104d7040cabca70d94a22b73fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab3ce48dc29d4d4bb838dd2d6f3d559f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_3897e5ee3e8d4022864b16db82959f5e","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14610be3ca1c40d193777716a0fd284c","value":456318}},"ac30452a79d048a2a8f4a825e9202cf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_db269fc5ac4142d2a6041e24eb339b95","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c40e1c6665054e32a5a1544d01612d7a","value":481}},"afde83aea74547f097b05332c1498592":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4d3ca74fdd04380ab975d906f94b553":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5d8fa159b664b32b62861aac6de1cd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a1cd0b84833438ba969b6c24afacef7","placeholder":"‚Äã","style":"IPY_MODEL_c8e1ba65268b48a9b7f3133f67560656","value":" 456k/456k [00:00&lt;00:00, 2.10MB/s]"}},"c1ee69ff85e94581ad8db4389ef30f0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c40e1c6665054e32a5a1544d01612d7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"c8e1ba65268b48a9b7f3133f67560656":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce18b1f07d5d46f0aef808d27de42937":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_f21a04c24ab341e9bd2ab96c62900761","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e204439c3c29401b84c396ebaab2e38e","value":501200538}},"d9080b485f7f41f1baba6d8a435ee099":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db269fc5ac4142d2a6041e24eb339b95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbb2ad99fc214361850496ac30ce73aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e204439c3c29401b84c396ebaab2e38e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"edfa9e55c5a14d5d8d226386b127c581":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_977e76a538a44f909b9d53684685f97f","placeholder":"‚Äã","style":"IPY_MODEL_6e0b90816d5f4fe88cb9fbee376d4f13","value":" 899k/899k [00:00&lt;00:00, 1.77MB/s]"}},"f21a04c24ab341e9bd2ab96c62900761":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Natural Language Processing with Disaster Tweets (Part3 -- RoBERTa)\n# 15. Training, fine-tuning RoBerta-base\n\n----------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## RoBERTa-base with ClassificationModel\nMy initial foray into model training involved the BERT architecture, which, while powerful, proved to be resource-intensive and complex to fine-tune effectively. This experience led us to explore RoBERTa, a variant of BERT that offers several advantages, particularly in terms of ease of use and training efficiency.\n\nThe BERT model, with its intricate architecture and numerous hyperparameters, required substantial effort and time to optimize. We invested considerable energy into hyperparameter tuning, including adjustments to the learning rate, batch size, and the addition of layers to enhance model capacity. Despite our dedication, the results were not as promising as anticipated, leading to frustration and a realization that the complexity of BERT was hindering our progress.\n\nIn contrast, RoBERTa presents a more streamlined approach to model training. By utilizing the ClassificationModel class, we were able to simplify the process of building and training our model significantly. This class abstracts many of the complexities associated with model configuration and hyperparameter tuning, allowing us to focus on the core aspects of our NLP task. The ease of implementation provided by RoBERTa not only reduced the time spent on model setup but also enabled us to achieve faster iterations and more effective experimentation.\n\nMoreover, RoBERTa's training methodology, which includes dynamic masking and a larger training dataset, enhances its performance on various NLP tasks. This robustness, combined with the user-friendly interface of the ClassificationModel class, allowed us to achieve competitive results with less effort compared to our previous experiences with BERT. The transition to RoBERTa has not only improved our workflow but has also reinvigorated our enthusiasm for model development.\n\nIn conclusion, our shift from BERT to RoBERTa exemplifies the importance of selecting the right tools and frameworks in the pursuit of effective NLP solutions. By leveraging the capabilities of the ClassificationModel class, we have streamlined our model training process, allowing us to focus on refining our approach and achieving better results. As we continue to explore the potential of RoBERTa, we are optimistic about the advancements we can make in our NLP projects, ultimately leading to more impactful outcomes in our research and applications.","metadata":{}},{"cell_type":"code","source":"!pip install transformers==2.11.0 --quiet\n!pip install pyspellchecker --quiet\n!pip install simpletransformers --quiet","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2025-02-23T22:15:52.048136Z","iopub.execute_input":"2025-02-23T22:15:52.048334Z","iopub.status.idle":"2025-02-23T22:16:43.550534Z","shell.execute_reply.started":"2025-02-23T22:15:52.048313Z","shell.execute_reply":"2025-02-23T22:16:43.549582Z"},"papermill":{"duration":30.967325,"end_time":"2020-11-14T04:18:39.890697","exception":false,"start_time":"2020-11-14T04:18:08.923372","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport torch\nimport numpy as np\nimport pandas as pd\nimport time\nimport simpletransformers\nfrom simpletransformers.classification import ClassificationModel\nimport warnings\nwarnings.simplefilter('ignore')\nfrom scipy.special import softmax\nimport sklearn\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss, f1_score\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 100)\n\ndef seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n\nseed_all(79)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():      \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:17:15.465750Z","iopub.execute_input":"2025-02-23T22:17:15.465988Z","iopub.status.idle":"2025-02-23T22:17:15.559877Z","shell.execute_reply.started":"2025-02-23T22:17:15.465962Z","shell.execute_reply":"2025-02-23T22:17:15.558924Z"},"papermill":{"duration":0.427202,"end_time":"2020-11-14T04:18:48.210478","exception":false,"start_time":"2020-11-14T04:18:47.783276","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparing Dataset for training","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/cleaned-data/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/cleaned-data/df_test.csv\")\nprint(\"Shape of train data : \",train.shape)\nprint(\"Shape of test data : \",test.shape)","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:17:15.847632Z","iopub.execute_input":"2025-02-23T22:17:15.847901Z","iopub.status.idle":"2025-02-23T22:17:15.983781Z","shell.execute_reply.started":"2025-02-23T22:17:15.847873Z","shell.execute_reply":"2025-02-23T22:17:15.983081Z"},"papermill":{"duration":0.085012,"end_time":"2020-11-14T04:18:48.442435","exception":false,"start_time":"2020-11-14T04:18:48.357423","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add the keyword column to the text column\ntrain['keyword'].fillna('', inplace=True)\ntrain['final_text'] = train['keyword'] + ' ' + train['text'] \ntest['keyword'].fillna('', inplace=True)\ntest['final_text'] = test['keyword'] + ' ' + test['text'] ","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:17:15.984524Z","iopub.execute_input":"2025-02-23T22:17:15.984743Z","iopub.status.idle":"2025-02-23T22:17:15.997167Z","shell.execute_reply.started":"2025-02-23T22:17:15.984716Z","shell.execute_reply":"2025-02-23T22:17:15.996389Z"},"papermill":{"duration":0.065151,"end_time":"2020-11-14T04:18:48.597919","exception":false,"start_time":"2020-11-14T04:18:48.532768","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_col = ['final_text']\nlast_cols = [col for col in train.columns if col not in first_col]\n\ntrain = train[first_col+last_cols]\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:17:15.997889Z","iopub.execute_input":"2025-02-23T22:17:15.998117Z","iopub.status.idle":"2025-02-23T22:17:16.041018Z","shell.execute_reply.started":"2025-02-23T22:17:15.998096Z","shell.execute_reply":"2025-02-23T22:17:16.040237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train=train.drop(['id'],axis=1)\ntrain=train.drop(['keyword'],axis=1)\ntrain=train.drop(['b4combine'],axis=1)\ntrain=train.drop(['b4embedding_text'],axis=1)\ntrain=train.drop(['word_count'],axis=1)\ntrain=train.drop(['unique_word_count'],axis=1)\ntrain=train.drop(['stop_word_count'],axis=1)\ntrain=train.drop(['mean_word_length'],axis=1)\ntrain=train.drop(['char_count'],axis=1)\ntrain=train.drop(['punctuation_count'],axis=1)\ntrain=train.drop(['text'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:17:16.041908Z","iopub.execute_input":"2025-02-23T22:17:16.042212Z","iopub.status.idle":"2025-02-23T22:17:16.057042Z","shell.execute_reply.started":"2025-02-23T22:17:16.042162Z","shell.execute_reply":"2025-02-23T22:17:16.056138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:17:16.057878Z","iopub.execute_input":"2025-02-23T22:17:16.058096Z","iopub.status.idle":"2025-02-23T22:17:16.065298Z","shell.execute_reply.started":"2025-02-23T22:17:16.058066Z","shell.execute_reply":"2025-02-23T22:17:16.064662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final=pd.DataFrame()\nfinal['id']=test['id']\nfinal.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:17:16.067895Z","iopub.execute_input":"2025-02-23T22:17:16.068084Z","iopub.status.idle":"2025-02-23T22:17:16.085660Z","shell.execute_reply.started":"2025-02-23T22:17:16.068066Z","shell.execute_reply":"2025-02-23T22:17:16.084998Z"},"papermill":{"duration":0.04089,"end_time":"2020-11-14T04:18:48.888859","exception":false,"start_time":"2020-11-14T04:18:48.847969","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_col = ['final_text']\nlast_cols = [col for col in test.columns if col not in first_col]\n\ntest = test[first_col+last_cols]\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:17:16.087177Z","iopub.execute_input":"2025-02-23T22:17:16.087429Z","iopub.status.idle":"2025-02-23T22:17:16.108587Z","shell.execute_reply.started":"2025-02-23T22:17:16.087408Z","shell.execute_reply":"2025-02-23T22:17:16.107957Z"},"papermill":{"duration":0.041622,"end_time":"2020-11-14T04:18:49.029412","exception":false,"start_time":"2020-11-14T04:18:48.987790","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test=test.drop(['id'],axis=1)\ntest=test.drop(['keyword'],axis=1)\ntest=test.drop(['text'],axis=1)\ntest=test.drop(['b4combine'],axis=1)\ntest=test.drop(['b4embedding_text'],axis=1)\ntest=test.drop(['word_count'],axis=1)\ntest=test.drop(['unique_word_count'],axis=1)\ntest=test.drop(['stop_word_count'],axis=1)\ntest=test.drop(['mean_word_length'],axis=1)\ntest=test.drop(['char_count'],axis=1)\ntest=test.drop(['punctuation_count'],axis=1)\ntest['label']=0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:17:16.109453Z","iopub.execute_input":"2025-02-23T22:17:16.109734Z","iopub.status.idle":"2025-02-23T22:17:16.130854Z","shell.execute_reply.started":"2025-02-23T22:17:16.109704Z","shell.execute_reply":"2025-02-23T22:17:16.130194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:17:16.131555Z","iopub.execute_input":"2025-02-23T22:17:16.131789Z","iopub.status.idle":"2025-02-23T22:17:16.138940Z","shell.execute_reply.started":"2025-02-23T22:17:16.131768Z","shell.execute_reply":"2025-02-23T22:17:16.138100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:17:16.139679Z","iopub.execute_input":"2025-02-23T22:17:16.139950Z","iopub.status.idle":"2025-02-23T22:17:16.158790Z","shell.execute_reply.started":"2025-02-23T22:17:16.139920Z","shell.execute_reply":"2025-02-23T22:17:16.158140Z"},"papermill":{"duration":0.040657,"end_time":"2020-11-14T04:18:49.099401","exception":false,"start_time":"2020-11-14T04:18:49.058744","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:17:16.159486Z","iopub.execute_input":"2025-02-23T22:17:16.159724Z","iopub.status.idle":"2025-02-23T22:17:16.174644Z","shell.execute_reply.started":"2025-02-23T22:17:16.159703Z","shell.execute_reply":"2025-02-23T22:17:16.173844Z"},"papermill":{"duration":0.041123,"end_time":"2020-11-14T04:18:49.170150","exception":false,"start_time":"2020-11-14T04:18:49.129027","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Target Imbalance Rate: 0 vs 1 = \", 4305/3198)","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:18:43.428807Z","iopub.execute_input":"2025-02-23T22:18:43.429136Z","iopub.status.idle":"2025-02-23T22:18:43.433106Z","shell.execute_reply.started":"2025-02-23T22:18:43.429112Z","shell.execute_reply":"2025-02-23T22:18:43.432297Z"},"papermill":{"duration":0.037115,"end_time":"2020-11-14T04:18:49.236642","exception":false,"start_time":"2020-11-14T04:18:49.199527","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train.reindex(np.random.permutation(train.index))\ntrain= train.reset_index(drop=True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:18:51.773770Z","iopub.execute_input":"2025-02-23T22:18:51.774059Z","iopub.status.idle":"2025-02-23T22:18:51.784030Z","shell.execute_reply.started":"2025-02-23T22:18:51.774034Z","shell.execute_reply":"2025-02-23T22:18:51.783342Z"},"papermill":{"duration":0.044285,"end_time":"2020-11-14T04:18:49.310266","exception":false,"start_time":"2020-11-14T04:18:49.265981","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom scipy.special import softmax","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:19:20.852759Z","iopub.execute_input":"2025-02-23T22:19:20.853053Z","iopub.status.idle":"2025-02-23T22:19:20.856539Z","shell.execute_reply.started":"2025-02-23T22:19:20.853029Z","shell.execute_reply":"2025-02-23T22:19:20.855610Z"},"papermill":{"duration":0.036783,"end_time":"2020-11-14T04:18:49.376986","exception":false,"start_time":"2020-11-14T04:18:49.340203","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1 = sklearn.metrics.f1_score","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:19:27.574625Z","iopub.execute_input":"2025-02-23T22:19:27.574949Z","iopub.status.idle":"2025-02-23T22:19:27.578620Z","shell.execute_reply.started":"2025-02-23T22:19:27.574920Z","shell.execute_reply":"2025-02-23T22:19:27.577880Z"},"papermill":{"duration":0.035783,"end_time":"2020-11-14T04:18:49.442080","exception":false,"start_time":"2020-11-14T04:18:49.406297","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Parameters Tuning\n*This is the current parameter settings of training a RoBERTa model for this classification project including some recommendation for the next potential improvement through parameter tuning.*\n\n- Epochs:  Two epochs may be insufficient for convergence, especially for complex models like RoBERTa. Consider increasing this to 3-5 epochs and monitor performance on the validation set.\n\n- Experimenting with the Learning Rate:  A learning rate of 2e-5 is a common starting point, but it may be beneficial to test different values (e.g., 1e-5, 3e-5) and consider using a learning rate scheduler to adapt the learning rate during training.\n\n- Considering Mixed Precision Training:  If your hardware supports it (e.g., NVIDIA GPUs), enabling mixed precision training (fp16: True) can speed up training and reduce memory usage.\n\n- Adjusting Class Weights & Monitoring Performance Metrics(F1 score):  The weight parameter is crucial for addressing class imbalance. The weights reflect the actual class distribution in this dataset.  F1 score:  F1 score is a critical metric for evaluating model performance, especially in the context of class imbalance. By monitoring the F1 score, you can gain insights into the model's ability to balance precision and recall for both classes. This is particularly important when the minority class is underrepresented, as a high overall accuracy may mask poor performance on that class. Regularly tracking the F1 score allows for timely adjustments to class weights and other parameters to ensure that the model is effectively learning from both classes.  By systematically tuning these parameters and evaluating their impact on model performance, you can enhance the effectiveness of the RoBERTa model for your specific classification task. This iterative process will help in achieving better generalization and improved predictive accuracy, particularly in scenarios where class imbalance is a significant concern.","metadata":{}},{"cell_type":"code","source":"# model configuration\nmodel_args = {\n    \"save_eval_checkpoints\": False,\n    \"save_model_every_epoch\": False,\n    'reprocess_input_data': True,\n    'overwrite_output_dir': True,\n    'manual_seed': 79,\n    \"silent\": True,\n    'num_train_epochs': 2,\n    'learning_rate': 2e-5,\n    'fp16': False,\n    'max_seq_length': 64,\n}","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:19:31.917334Z","iopub.execute_input":"2025-02-23T22:19:31.917631Z","iopub.status.idle":"2025-02-23T22:19:31.921727Z","shell.execute_reply.started":"2025-02-23T22:19:31.917604Z","shell.execute_reply":"2025-02-23T22:19:31.920856Z"},"papermill":{"duration":0.038975,"end_time":"2020-11-14T04:18:49.511609","exception":false,"start_time":"2020-11-14T04:18:49.472634","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train.columns) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:19:34.326924Z","iopub.execute_input":"2025-02-23T22:19:34.327251Z","iopub.status.idle":"2025-02-23T22:19:34.331701Z","shell.execute_reply.started":"2025-02-23T22:19:34.327215Z","shell.execute_reply":"2025-02-23T22:19:34.330975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train['final_text'].head())  # Check the first few entries\nprint(train['final_text'].apply(type))  # Check types of entries\nprint(test['final_text'].head())  # Check the first few entries\nprint(test['final_text'].apply(type))  # Check types of entries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:19:39.862894Z","iopub.execute_input":"2025-02-23T22:19:39.863210Z","iopub.status.idle":"2025-02-23T22:19:39.874997Z","shell.execute_reply.started":"2025-02-23T22:19:39.863163Z","shell.execute_reply":"2025-02-23T22:19:39.874105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['final_text'].fillna(\"\", inplace=True)  # Replace NaN with empty strings\ntest['final_text'].fillna(\"\", inplace=True)  # Replace NaN with empty strings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:19:54.542040Z","iopub.execute_input":"2025-02-23T22:19:54.542366Z","iopub.status.idle":"2025-02-23T22:19:54.548012Z","shell.execute_reply.started":"2025-02-23T22:19:54.542338Z","shell.execute_reply":"2025-02-23T22:19:54.547337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['final_text'] = train['final_text'].astype(str)  # Convert all entries to string\ntest['final_text'] = test['final_text'].astype(str)  # Convert all entries to string","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T22:19:56.869162Z","iopub.execute_input":"2025-02-23T22:19:56.869481Z","iopub.status.idle":"2025-02-23T22:19:56.874592Z","shell.execute_reply.started":"2025-02-23T22:19:56.869456Z","shell.execute_reply":"2025-02-23T22:19:56.873854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model information (RoBERTa-base)\n- RoBERTa is an advanced version of BERT (Bidirectional Encoder Representations from Transformers) developed by Facebook AI in 2019. \n- It was introduced in the paper:   üìÑ RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019).\n- RoBERTa builds upon BERT but makes several improvements in the way the model is pre-trained, leading to better performance on many NLP tasks.\n\n-------------------------------\nüî• How is RoBERTa Different from BERT?     \n*RoBERTa improves BERT in several key ways:*\n\n1) Trained on More Data üìä\n- RoBERTa is trained on 160GB of text data, compared to BERT‚Äôs 16GB.\n- This extra data includes Common Crawl, BooksCorpus, OpenWebText, and Wikipedia.\n2) Removes Next Sentence Prediction (NSP) ‚ùå\n- BERT uses Next Sentence Prediction (NSP) during pre-training.\n- RoBERTa removes NSP, which was found to be unnecessary and even detrimental.\n3) Uses More Data for Masked Language Modeling (MLM) üîÑ\n- In BERT, 15% of words are masked once per epoch.\n- RoBERTa dynamically changes the masked words in every iteration, helping the model generalize better.\n4) Bigger Batches and Longer Training üèãÔ∏è‚Äç‚ôÇÔ∏è\n- RoBERTa is trained for more steps and with larger batch sizes compared to BERT.\n- BERT‚Äôs max batch size: 256 sequences\n- RoBERTa‚Äôs max batch size: 8,000 sequences\n5) Better Hyperparameter Tuning üéØ\n- The learning rate, batch size, and training schedules are optimized for better results.\n\n\nüìä Performance: How Well Does RoBERTa Perform?\n\n- RoBERTa generally outperforms BERT across various NLP benchmarks, including the GLUE tasks, where RoBERTa achieved a score of 88.5 compared to BERT's lower performance. Its enhanced training methods, such as dynamic masking and a larger dataset, contribute to its superior ability to understand language complexities. RoBERTa has demonstrated significant improvements over BERT in several key benchmarks:\n\n- SQuAD (Stanford Question Answering Dataset): RoBERTa achieved an F1 score of 94.6, surpassing BERT's score of 93.2. This indicates a notable enhancement in question-answering capabilities.\n\n- GLUE (General Language Understanding Evaluation): RoBERTa scored 88.5, while BERT managed only 84.6, showcasing its superior performance across various language understanding tasks.\n\n- Named Entity Recognition (NER): RoBERTa excels in extracting complex entities from unstructured text, outperforming BERT in recognizing names of people, places, and organizations.\n\n- Sentiment Analysis: RoBERTa's fine-tuned models are particularly effective in detecting subtle emotions in text, outperforming BERT in classifying sentiments accurately.\n\nThese benchmarks highlight RoBERTa's advancements in natural language processing, making it a preferred choice for many applications requiring high accuracy and efficiency.  RoBERTa is more accurate than BERT for many tasks like text classification, question answering, and sentiment analysis.","metadata":{}},{"cell_type":"markdown","source":"## Training with ClassificationModel Class\n\nThe ClassificationModel class in simpletransformers is designed to make it easier to work with transformer-based models for text classification tasks. By specifying the model type, pre-trained weights, and training arguments, you can quickly set up and fine-tune a model for your specific classification needs.\n\n- Model Type: The ClassificationModel allows you to work with various transformer architectures, such as BERT, RoBERTa, DistilBERT, and more, by specifying the model type (e.g., 'roberta' for RoBERTa).\n- Pre-trained Model: You can specify a pre-trained model checkpoint from Hugging Face's Model Hub (e.g., 'roberta-base'), which contains weights trained on large corpora and can be fine-tuned for specific tasks like classification.\n\n1) Model Type:\n- The first parameter (e.g., 'roberta') specifies the type of model you want to use for classification. This indicates that the model will leverage RoBERTa architecture.\n\n2) Model Name or Path:\n- The second parameter (e.g., 'roberta-base') is the name of the pre-trained model or a path to a local model directory. This is where the model's weights and configuration will be loaded from.\n\n3) Weight:\n- The weight parameter (optional) allows you to set class weights for imbalanced datasets. It can help the model pay more attention to underrepresented classes during training. In your example, weight=[1, 1.346] indicates the relative importance of two classes.\n\n4) Arguments (args):\n- The args parameter allows you to specify training arguments such as learning rate, batch size, number of epochs, and more. This is typically a dictionary with keys that correspond to the training options.\n","metadata":{}},{"cell_type":"code","source":"import time\nstart_time = time.time()\n# Clear CUDA cache\ntorch.cuda.empty_cache()\n\n# Set up Stratified K-Fold\nkf = StratifiedKFold(n_splits=15, shuffle=True, random_state=79)\nerr = []\ny_pred_tot = []\n\n# Perform Stratified K-Fold cross-validation\nfor train_index, test_index in kf.split(train, train['target']):\n    train1_trn, train1_val = train.iloc[train_index], train.iloc[test_index]\n    \n    # Initialize the model\n    model_rb = ClassificationModel('roberta', 'roberta-base', weight=[1, 1.346], args=model_args)\n    \n    # Train the model\n    model_rb.train_model(train1_trn, eval_df=train1_val)\n\n    # Evaluate the model\n    result, model_outputs, _ = model_rb.eval_model(train1_val, f1=sklearn.metrics.f1_score, acc=sklearn.metrics.accuracy_score)\n    print(f\"F1 Score: {result['f1']:.4f}, Accuracy: {result['acc']:.4f}\")    \n    \n    err.append(result['f1'])\n\n    # Make predictions\n    try:\n        predictions, _ = model_rb.predict(test['final_text'].tolist())  # Convert to list\n        y_pred_tot.append(predictions)\n        \n    except Exception as e:\n        print(\"Error during prediction:\", e)\n    \n    # Print mean F1 score after each fold\n    print(\"Mean F1 Score: \", np.mean(err))\n\n# Final mean F1 score across all folds\nprint(\"Final Mean F1 Score: \", np.mean(err))\n\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2025-02-23T22:23:29.646561Z","iopub.execute_input":"2025-02-23T22:23:29.646894Z","iopub.status.idle":"2025-02-23T22:58:48.616408Z","shell.execute_reply.started":"2025-02-23T22:23:29.646860Z","shell.execute_reply":"2025-02-23T22:58:48.615308Z"},"papermill":{"duration":2805.837979,"end_time":"2020-11-14T05:05:35.382616","exception":false,"start_time":"2020-11-14T04:18:49.544637","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 16. Prediction","metadata":{}},{"cell_type":"code","source":"target_submit =np.mean(y_pred_tot,0)\nprint(target_submit[100:150])","metadata":{"execution":{"iopub.status.busy":"2025-02-23T23:09:03.948645Z","iopub.execute_input":"2025-02-23T23:09:03.948961Z","iopub.status.idle":"2025-02-23T23:09:03.954571Z","shell.execute_reply.started":"2025-02-23T23:09:03.948935Z","shell.execute_reply":"2025-02-23T23:09:03.953824Z"},"papermill":{"duration":0.045762,"end_time":"2020-11-14T05:05:35.466616","exception":false,"start_time":"2020-11-14T05:05:35.420854","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_submit","metadata":{"execution":{"iopub.status.busy":"2025-02-23T23:09:05.819028Z","iopub.execute_input":"2025-02-23T23:09:05.819383Z","iopub.status.idle":"2025-02-23T23:09:05.824887Z","shell.execute_reply.started":"2025-02-23T23:09:05.819351Z","shell.execute_reply":"2025-02-23T23:09:05.824033Z"},"papermill":{"duration":0.046734,"end_time":"2020-11-14T05:05:35.551695","exception":false,"start_time":"2020-11-14T05:05:35.504961","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#predictions, raw_outputs = model_rb.predict(test['final_text'])\nfinal['target']=target_submit\nfinal['target'] = final['target'].apply(lambda x: 1 if x>0.5 else 0)\nfinal.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-23T23:09:11.850722Z","iopub.execute_input":"2025-02-23T23:09:11.851044Z","iopub.status.idle":"2025-02-23T23:09:11.860910Z","shell.execute_reply.started":"2025-02-23T23:09:11.851016Z","shell.execute_reply":"2025-02-23T23:09:11.860161Z"},"papermill":{"duration":0.063333,"end_time":"2020-11-14T05:05:35.656702","exception":false,"start_time":"2020-11-14T05:05:35.593369","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2025-02-23T23:09:20.058643Z","iopub.execute_input":"2025-02-23T23:09:20.058922Z","iopub.status.idle":"2025-02-23T23:09:20.071841Z","shell.execute_reply.started":"2025-02-23T23:09:20.058901Z","shell.execute_reply":"2025-02-23T23:09:20.070972Z"},"papermill":{"duration":0.14525,"end_time":"2020-11-14T05:05:35.845379","exception":false,"start_time":"2020-11-14T05:05:35.700129","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/submis/submission.csv\")\nsubmission['target'] = submission['target'].apply(lambda x: 1 if x>0.5 else 0)\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T23:22:54.949800Z","iopub.execute_input":"2025-02-23T23:22:54.950201Z","iopub.status.idle":"2025-02-23T23:22:54.965797Z","shell.execute_reply.started":"2025-02-23T23:22:54.950162Z","shell.execute_reply":"2025-02-23T23:22:54.964869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T23:23:06.887944Z","iopub.execute_input":"2025-02-23T23:23:06.888348Z","iopub.status.idle":"2025-02-23T23:23:06.901801Z","shell.execute_reply.started":"2025-02-23T23:23:06.888318Z","shell.execute_reply":"2025-02-23T23:23:06.900683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 17. Submission Result\n\n\n![Image](https://github.com/user-attachments/assets/c685acef-2ea0-4bde-bcb3-0b8e3d8514d9)","metadata":{}},{"cell_type":"markdown","source":"# 18. Conclusion ","metadata":{}},{"cell_type":"markdown","source":"### *After Fine-Tuning BERT, and then RoBERTa for Disaster Prediction Using Twitter Text*\n- This project aimed to enhance disaster prediction capabilities by fine-tuning BERT and RoBERTa models on Twitter text data. Our results indicated that both models effectively identified disaster-related tweets, with RoBERTa consistently a little outperforming BERT in accuracy, precision, and recall metrics. \n\n- Future plans include exploring additional data augmentation techniques to improve model robustness and experimenting with ensemble methods that combine predictions from both models. We also aim to incorporate sentiment analysis to better understand public sentiment during disasters, which could enhance the predictive capabilities of our models. Continuous evaluation and adaptation of our models will be essential as we gather more diverse and real-time data from Twitter. Project Conclusion Report: Fine-Tuning BERT and RoBERTa for Disaster Prediction Using Twitter Text\n\n- In this project, we focused on predicting disasters through binary classification of Twitter text by fine-tuning BERT and RoBERTa models. The results showed that BERT achieved an accuracy of **0.82071**, while RoBERTa slightly outperformed it with an accuracy of **0.83144**. Although the use of transformer architecture was deemed appropriate for understanding the relationships between words in the text, the score of 0.83144 was not entirely satisfactory.\n\n- The relatively low performance can be attributed to the limited dataset of approximately 7,000 tweets, which is significantly smaller compared to the vast datasets used to train large language models (LLMs). To address this, we are confident that acquiring a larger dataset of tweets will enable us to reach a score of 1 in future iterations.\n\n- Additionally, we believe that developing a custom transformer model tailored to our specific needs could also bring us closer to achieving this goal. By leveraging more extensive and diverse data, we aim to enhance the model's predictive capabilities and overall performance in disaster prediction tasks.\n\n- In conclusion, the project has laid a solid foundation for future work, and we are optimistic about the potential improvements that can be made with more data and refined modeling techniques.","metadata":{}}]}