{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10820614,"sourceType":"datasetVersion","datasetId":6718272},{"sourceId":10830406,"sourceType":"datasetVersion","datasetId":6725109},{"sourceId":10833186,"sourceType":"datasetVersion","datasetId":6727192}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Natural Language Processing with Disaster Tweets (Part2 -- BERT)","metadata":{}},{"cell_type":"markdown","source":"# 5. Training, fine-tuning Bert-base\n\n-----------------------------","metadata":{}},{"cell_type":"markdown","source":"# BERT-base with Classification Model\n\nIn this BERT model experiment, the first setup (with 1 dropout layer and 1 dense layer) performed better than the second setup (with 2 dropout layers and 3 dense layers). Here's how we can interpret the results from different perspectives:\n\n1. Increase in Model Complexity\nIn the second experiment, you added more dropout and dense layers (2 dropout layers and 3 dense layers). Adding more layers increases the number of parameters in the model, which can allow it to learn more complex patterns. However, this also increases the risk of overfitting. A more complex model may fit the training data too closely and fail to generalize well to unseen data. In contrast, the simpler model in the first experiment may have been better at generalizing to new data.\n\n2. Role of Dropout Layers\nDropout is a regularization technique that helps prevent overfitting by randomly ignoring certain neurons during training. However, adding too many dropout layers can make training unstable. In the first experiment, having just 1 dropout layer likely provided a good balance of regularization without causing too much information loss. In the second experiment, increasing the dropout layers to 2 might have overly constrained the model, leading to performance degradation due to too much regularization.\n\n3. Adding Dense Layers\nAdding dense layers increases the representational power of the model, but too many layers can make the learning process more challenging or cause issues like gradient vanishing. The first experiment, with only 1 dense layer, might have been sufficient to capture the important patterns in the data. In contrast, adding more layers in the second experiment might have made the model unnecessarily complex, making it harder for the model to learn effectively, and possibly leading to performance loss.\n\n4. Learning Rate and Initialization Issues\nWhen changing the model architecture, the learning rate or parameter initialization may need to be adjusted. In the second experiment, with a more complex architecture, it‚Äôs possible that the learning rate or initialization wasn‚Äôt ideal for the new setup, leading to poorer performance.\n\n\n*(One thing I regret is not properly saving the notebook information from the first experiment, so I can only submit the notebook from the second experiment.)*","metadata":{}},{"cell_type":"code","source":"!pip install transformers -q\n!pip install transformers datasets torch -q\n!pip install torch torchvision torchaudio -q\n\nprint(\"all done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T17:29:17.832086Z","iopub.execute_input":"2025-02-23T17:29:17.832362Z","iopub.status.idle":"2025-02-23T17:29:29.802445Z","shell.execute_reply.started":"2025-02-23T17:29:17.832341Z","shell.execute_reply":"2025-02-23T17:29:29.801177Z"}},"outputs":[{"name":"stdout","text":"all done\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport string\nimport operator\nimport urllib.request\nimport zipfile\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 100)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\n\nfrom transformers import BertForSequenceClassification\nfrom transformers import BertModel\nfrom transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\n\nimport torch\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom datasets import load_dataset\nfrom wordcloud import STOPWORDS\n\nimport nltk\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, GroupKFold, train_test_split, GroupShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch.nn.functional as F\nfrom datasets import load_dataset\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T17:29:29.803498Z","iopub.execute_input":"2025-02-23T17:29:29.803833Z","iopub.status.idle":"2025-02-23T17:29:53.484654Z","shell.execute_reply.started":"2025-02-23T17:29:29.803809Z","shell.execute_reply":"2025-02-23T17:29:53.483975Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed):\n    import random\n    SEED=seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T10:45:32.490149Z","iopub.execute_input":"2025-02-23T10:45:32.490452Z","iopub.status.idle":"2025-02-23T10:45:32.554872Z","shell.execute_reply.started":"2025-02-23T10:45:32.490429Z","shell.execute_reply":"2025-02-23T10:45:32.553995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check and set device (using GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:26.940891Z","iopub.execute_input":"2025-02-23T11:01:26.941341Z","iopub.status.idle":"2025-02-23T11:01:26.946337Z","shell.execute_reply.started":"2025-02-23T11:01:26.941309Z","shell.execute_reply":"2025-02-23T11:01:26.945630Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Encoding with Tokenizer","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, targets, max_len):\n        self.texts = texts\n        self.targets = targets  # ‚úÖ targetsÍ∞Ä NoneÏùº ÏàòÎèÑ ÏûàÏùå\n        self.max_len =  max_len\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n            return_token_type_ids=True  # ‚úÖ Ï∂îÍ∞ÄÎê®\n        )\n        \n        # ÏòàÏ∏° Ïãú targetsÍ∞Ä NoneÏùº Ïàò ÏûàÏúºÎØÄÎ°ú Ï°∞Í±¥Î∂ÄÎ°ú Î∞òÌôò\n        item = {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"token_type_ids\": encoding[\"token_type_ids\"].squeeze(0),  # ‚úÖ Ìè¨Ìï®\n        }\n        \n        if self.targets is not None:\n            item[\"labels\"] = torch.tensor(self.targets[idx], dtype=torch.long)\n        \n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:41.717639Z","iopub.execute_input":"2025-02-23T11:01:41.718019Z","iopub.status.idle":"2025-02-23T11:01:41.724859Z","shell.execute_reply.started":"2025-02-23T11:01:41.717988Z","shell.execute_reply":"2025-02-23T11:01:41.723848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Model building on top of Bert \nImplemented a custom BertClassifier class using the BERT pre-trained layer and incorporating custom new layers (dropout layers, and dense layers), and a sigmoid activation function.\n\nFor reference, BERT-Base Uncased model (bert-en-uncased-l-12-h-768-a-12/2) has 12-layer, 768-hidden, 12-heads, 110M parameters:\n\n- 12-layer: The BERT-Base model consists of 12 transformer encoder layers. Each layer extracts high-dimensional features from the input text to contribute to context understanding.\n- 768-hidden: The hidden size (or unit) of each layer is 768. This means that each word is ultimately represented by 768 numbers, which contain the meaning and context of the word.\n- 12-heads: Each encoder layer has 12 attention heads. Multi-head attention allows the model to capture information from different aspects of the text. For example, it can focus on grammar, meaning, and structure differently.\n- 110M parameters: This model has about 110 million parameters, which indicates the amount and complexity of information learned by the model.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\nclass BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        \n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False  # BERT Í∞ÄÏ§ëÏπò ÏóÖÎç∞Ïù¥Ìä∏ Î∞©ÏßÄ (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\n\n        self.dropout1 = nn.Dropout(0.1)\n        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)\n        self.dropout2 = nn.Dropout(0.1)\n        self.fc2 = nn.Linear(256, 32)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        clf_output = outputs.pooler_output  # [CLS]Ïùò Î≥ÄÌôòÍ∞í\n        x = self.dropout1(clf_output)\n        x = self.fc1(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x  # Sigmoid Ï†úÍ±∞ (loss Ìï®ÏàòÏóêÏÑú Ï≤òÎ¶¨)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:45.566089Z","iopub.execute_input":"2025-02-23T11:01:45.566371Z","iopub.status.idle":"2025-02-23T11:01:45.572674Z","shell.execute_reply.started":"2025-02-23T11:01:45.566350Z","shell.execute_reply":"2025-02-23T11:01:45.571931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Define Metrix to Measure","metadata":{}},{"cell_type":"markdown","source":"### Explanation of Training Metrics  \n\n- **Training Precision**:  \n  This measures the proportion of **true positive predictions** out of all positive predictions made by the model. It evaluates how many of the predicted positive cases were actually correct, focusing on minimizing false positives (FP). A higher precision indicates a lower rate of incorrect positive predictions.  \n\n  \\\n  \\begin{equation}\n  \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n  \\end{equation}\n    \n\n- **Training Recall**:  \n  This metric assesses the proportion of **actual positive cases** that were correctly predicted by the model. It evaluates how well the model identifies all relevant instances while minimizing false negatives (FN). A higher recall indicates fewer missed positive cases.  \n\n  \\\n  \\begin{equation}\n  \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n  \\end{equation}\n    \n\n- **Training F1 Score**:  \n  The F1 score is the **harmonic mean** of precision and recall, balancing both metrics. It is particularly useful when dealing with imbalanced datasets, as it ensures that neither precision nor recall is disproportionately prioritized.  \n\n  \\\n  \\begin{equation}\n  \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n  \\end{equation}\n    \n\n- **Training Accuracy**:  \n  Accuracy represents the proportion of **correct predictions** (both positive and negative) out of all predictions made by the model. While useful in balanced datasets, it may not always be a reliable metric in highly imbalanced cases.  \n\n  \\\n  \\begin{equation}\n  \\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n  \\end{equation}\n    \n\n- **Training Loss**:  \n  The loss function measures the **difference between the predicted output and the actual label**. It is used to optimize the model during training, and lower values indicate better performance. Common loss functions include Cross-Entropy Loss (for classification) and Mean Squared Error (for regression).\n","metadata":{}},{"cell_type":"code","source":"class ClassificationReport:\n    def __init__(self):\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        self.train_accuracy_scores = []  # Added list for accuracy\n        self.train_loss = []\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = []\n        self.val_accuracy_scores = []  # Added list for accuracy\n        self.val_loss = []\n\n    def on_epoch_end(self, model, train_loader, val_loader, device, criterion):\n        model.eval()\n        \n        # üîπ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÌèâÍ∞Ä\n        train_preds, train_labels, train_loss = self._predict_with_loss(model, train_loader, device, criterion)\n        train_precision = precision_score(train_labels, train_preds, average='macro')\n        train_recall = recall_score(train_labels, train_preds, average='macro')\n        train_f1 = f1_score(train_labels, train_preds, average='macro')\n        train_accuracy = np.mean(train_preds == train_labels)  # Accuracy calculation\n\n        self.train_precision_scores.append(train_precision)\n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        self.train_accuracy_scores.append(train_accuracy)  # Store accuracy\n        self.train_loss.append(train_loss)\n\n        # üîπ Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ ÌèâÍ∞Ä\n        val_preds, val_labels, val_loss = self._predict_with_loss(model, val_loader, device, criterion)\n        val_precision = precision_score(val_labels, val_preds, average='macro')\n        val_recall = recall_score(val_labels, val_preds, average='macro')\n        val_f1 = f1_score(val_labels, val_preds, average='macro')\n        val_accuracy = np.mean(val_preds == val_labels)  # Accuracy calculation\n\n        self.val_precision_scores.append(val_precision)\n        self.val_recall_scores.append(val_recall)\n        self.val_f1_scores.append(val_f1)\n        self.val_accuracy_scores.append(val_accuracy)  # Store accuracy\n        self.val_loss.append(val_loss)\n\n        # üîπ EpochÎ≥Ñ Ï†êÏàò Ï∂úÎ†•\n        print(f'- Training Precision: {train_precision:.6f} - Training Recall: {train_recall:.6f} - Training F1: {train_f1:.6f} - Training Accuracy: {train_accuracy:.6f} - Training Loss: {train_loss:.6f}')\n        print(f'- Validation Precision: {val_precision:.6f} - Validation Recall: {val_recall:.6f} - Validation F1: {val_f1:.6f} - Validation Accuracy: {val_accuracy:.6f} - Validation Loss: {val_loss:.6f}')\n\n        # üîπ CUDA Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n        torch.cuda.empty_cache()\n\n    def _predict_with_loss(self, model, loader, device, criterion):\n        all_preds = []\n        all_labels = []\n        total_loss = 0.0\n        with torch.no_grad():\n            for batch in loader:\n                inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n                labels = batch['labels'].to(device).float().unsqueeze(1)  # (batch_size, 1)Î°ú Î≥ÄÌôò\n\n                outputs = model(**inputs)\n                loss = criterion(outputs, labels)    # Ïù¥Ï†ú labelsÎäî float ÌÉÄÏûÖÏù¥ÎØÄÎ°ú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÏßÄ ÏïäÏùå\n                total_loss += loss.item()\n\n                # üîπ `sigmoid` Ï†ÅÏö© ÌõÑ `round()` ÏàòÌñâ (Binary Classification)\n                preds = torch.sigmoid(outputs).cpu().numpy()\n                preds = np.round(preds)  # 0 ÎòêÎäî 1Î°ú Î≥ÄÌôò\n\n                all_preds.extend(preds)\n                all_labels.extend(labels.cpu().numpy())\n\n        return np.array(all_preds), np.array(all_labels), total_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:50.356808Z","iopub.execute_input":"2025-02-23T11:01:50.357210Z","iopub.status.idle":"2025-02-23T11:01:50.372184Z","shell.execute_reply.started":"2025-02-23T11:01:50.357176Z","shell.execute_reply":"2025-02-23T11:01:50.371224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Training Model","metadata":{}},{"cell_type":"markdown","source":"- For effective *hyperparameter tuning*, it is crucial to integrate $ptim.lr_scheduler$ in combination with a predefined learning rate schedule that adjusts dynamically based on patience intervals. This ensures that the model adapts to different training phases by fine-tuning the learning rate according to performance fluctuations.\n\n- Interestingly, during the fine-tuning process of the BERT model, I discovered that it achieved optimal performance when initialized with an exceptionally low learning rate. This was a rare yet significant observation, as such an approach was not commonly required for fine-tuning other models. This insight highlights the sensitivity of BERT to learning rate adjustments and underscores the importance of careful tuning to maximize its potential.","metadata":{}},{"cell_type":"code","source":"class DisasterDetector:\n    def __init__(self, max_seq_length, lr, epochs, batch_size, patience, model=None):\n        self.model = model if model is not None else BertClassifier()  # Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n        self.max_seq_length = max_seq_length\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.patience = patience\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.models = []\n        self.scores = {}\n        self.best_model = None  # ÌïôÏäµÎêú Î™®Îç∏ÏùÑ Ï†ÄÏû•Ìï† Î≥ÄÏàò\n        self.best_model = BertClassifier().to(self.device)  # Ïô∏Î∂ÄÏóêÏÑú Ï£ºÏñ¥ÏßÑ Î™®Îç∏ÏùÑ ÏÇ¨Ïö©\n\n        if model:\n            self.best_model = model.to(self.device)  # üîπ Ï£ºÏñ¥ÏßÑ Î™®Îç∏ ÏÇ¨Ïö©\n\n        # üîπ best_modelÏù¥ ÏóÜÏúºÎ©¥ Ï†ÄÏû•Îêú Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n        if self.best_model is None and os.path.exists(\"/kaggle/input/bert-v0224/best_model.pth\"):\n            try:\n                state_dict = torch.load(\"/kaggle/input/bert-v0224/best_model.pth\", map_location=self.device)\n                self.best_model = BertClassifier().to(self.device)\n                self.best_model.load_state_dict(state_dict)\n                print(\"‚úÖ Model loaded successfully from 'best_model.pth'\")\n\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Model loading failed: {e}\")\n                self.best_model = None  # Î™®Îç∏ Î°úÎìú Ïã§Ìå® Ïãú NoneÏúºÎ°ú ÏÑ§Ï†ï\n        else:\n            print(\"‚ö†Ô∏è No trained model found. Train the model first!\")\n                        \n    \n    def train(self, df_train):\n        skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n        best_accuracy = -np.inf \n        best_f1_score = -np.inf\n        best_model_state = None\n        patience_counter = 0\n        torch.cuda.empty_cache()  # ‚úÖ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n        \n        device = self.device  \n\n        ### üîπ ÌïôÏäµÏùÑ ÏúÑÌïú Î™®Îç∏ÏùÄ self.best_modelÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ≥†, ÏÉàÎ°úÏö¥ Î™®Îç∏ÏùÑ ÏÉùÏÑ±\n        model = BertClassifier(freeze_bert=True).to(device)  \n        model = torch.compile(model)  # ‚úÖ Î™®Îç∏ Ïª¥ÌååÏùº Ï†ÅÏö© (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\n\n        for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train['text'], df_train['target'])):\n            print(f'\\n.....[Fold {fold}].....\\n')\n            \n            train_dataset = TextDataset(df_train.loc[trn_idx, 'text'].values, df_train.loc[trn_idx, 'target'].values, max_len=self.max_seq_length)  # TextDataset(texts, labels, max_len) ÌòïÌÉúÎ°ú Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨\n            val_dataset = TextDataset(df_train.loc[val_idx, 'text'].values, df_train.loc[val_idx, 'target'].values, max_len=self.max_seq_length)\n            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True) # collate_fn=collate_fn\n            val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)  # collate_fn=collate_fn\n            print(\"Train/Val dataset are loaded...\")\n\n            optimizer = optim.Adam(model.parameters(), lr=self.lr, weight_decay=1e-5)\n            criterion = nn.BCEWithLogitsLoss()\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.3, verbose=True)\n            #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr, steps_per_epoch=len(train_loader), epochs=self.epochs)\n\n            metrics = ClassificationReport()\n            learning_rate = [0.05, 0.02, 0.105, 0.1, 0.2]  # ÎØ∏Î¶¨ ÏßÄÏ†ïÌïú ÌïôÏäµÎ•† Î¶¨Ïä§Ìä∏\n            lr_adjustment_count = 0  # ÌòÑÏû¨ ÌïôÏäµÎ•† Î≥ÄÍ≤Ω ÌöüÏàò\n            best_f1_score = 0\n            best_accuracy = 0\n            patience_counter = 0\n\n            for epoch in range(self.epochs):\n                print(f'Epoch: {epoch+1}')\n                model.train()\n                \n                for batch in train_loader:\n                    optimizer.zero_grad()\n                    inputs = {\"input_ids\": batch[\"input_ids\"].to(self.device), \"attention_mask\": batch[\"attention_mask\"].to(self.device), \"token_type_ids\": batch[\"token_type_ids\"].to(self.device)}  # ‚úÖ token_type_ids Ï∂îÍ∞Ä\n                    labels = batch['labels'].to(self.device).unsqueeze(1).float()\n                    outputs = model(**inputs)\n                    loss = criterion(outputs.view(-1, 1), labels)\n                    loss.backward()\n                    optimizer.step()\n\n                metrics.on_epoch_end(model, train_loader, val_loader, self.device, criterion)\n                val_loss = metrics.val_loss[-1]  # ÎßàÏßÄÎßâ Í≤ÄÏ¶ù ÏÜêÏã§ÏùÑ scalarÎ°ú Ï†ÄÏû•\n                scheduler.step(val_loss)  # Ïä§ÏπºÎùº Í∞íÏùÑ Ï†ÑÎã¨\n                val_accuracy = metrics.val_accuracy_scores[-1]\n                val_f1 = metrics.val_f1_scores[-1]  # Î∂àÍ∑†Ìòï Îç∞Ïù¥ÌÑ∞Ïùº Í≤ΩÏö∞ F1-scoreÎ•º Í∏∞Ï§ÄÏúºÎ°ú ÌïòÎäî Í≤ÉÏù¥ Îçî Ï†ÅÏ†àÌï®\n\n                scheduler.step(val_loss)  # üîπ ReduceLROnPlateau Ïã§Ìñâ\n\n                if val_f1 > best_f1_score:  # Best Î™®Îç∏ ÏóÖÎç∞Ïù¥Ìä∏\n                    best_f1_score = val_f1\n                    best_model_state = model.state_dict()\n                    print(\"Best f1 score is updated!\")\n\n                if val_accuracy > best_accuracy:  # Early Stopping Í∏∞Ï§Ä Ï≤¥ÌÅ¨\n                    best_accuracy = val_accuracy\n                    patience_counter = 0  \n                    print(\"Best accuracy updated!\")\n                else:\n                    patience_counter += 1  \n\n                if patience_counter >= self.patience:\n                    current_lr = optimizer.param_groups[0]['lr']\n                    \n                    # üîπ schedulerÍ∞Ä Ïù¥ÎØ∏ LRÏùÑ Ï§ÑÏó¨ÏÑú ÎÑàÎ¨¥ ÏûëÏïÑÏ°åÎã§Î©¥, Ïö∞Î¶¨Í∞Ä ÏßÅÏ†ë Î≥ÄÍ≤Ω\n                    if current_lr < 1e-6 and lr_adjustment_count < len(learning_rate):\n                        new_lr = learning_rate[lr_adjustment_count]  \n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = new_lr  # ÌïôÏäµÎ•† Î≥ÄÍ≤Ω\n                            patience_counter = 0  # patience Ï¥àÍ∏∞Ìôî ÌõÑ Îã§Ïãú ÏãúÎèÑ\n                            lr_adjustment_count += 1\n                            print(f\"Learning rate manually adjusted! New LR: {new_lr}, Attempts left: {len(learning_rate) - lr_adjustment_count}\")\n                    elif current_lr < 1e-6 and lr_adjustment_count >= len(learning_rate):\n                        print(\"Early stopping triggered based on accuracy & LR tuning exhausted!\")\n                        break\n        # üîπ ÌïôÏäµÏù¥ ÎÅùÎÇú ÌõÑ, self.best_modelÏóê Í∞ÄÏû• Ï¢ãÏùÄ Í∞ÄÏ§ëÏπòÎ•º Î°úÎìú\n        if best_model_state:\n            self.best_model = BertClassifier().to(self.device)\n            self.best_model.load_state_dict(best_model_state, strict=False)\n            torch.save(self.best_model.state_dict(), \"best_model.pth\")  # üîπ Î™®Îç∏ Í∞ÄÏ§ëÏπò Ï†ÄÏû•\n            print(\"Best model is saved in 'best_model.pth'\")\n           \n\n    def predict(self, df_test):\n        if self.best_model is None:\n            raise ValueError(\"‚ùå No trained model found. Train the model first!\")\n            \n    \n    self.best_model.eval()  # Î™®Îç∏Ïù¥ NoneÏù¥ ÏïÑÎãàÎùºÎäî Í≤å Î≥¥Ïû•Îê®\n    test_dataset = TextDataset(df_test['text'].values, None, max_len=self.max_seq_length)\n    test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            # ÌïÑÏöîÌïú ÏûÖÎ†•Îßå Í∞ÄÏ†∏Ïò§Í∏∞\n            inputs = {key: val.to(self.device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n            outputs = self.best_model(**inputs)\n            \n            # logitsÏóê sigmoid Ï†ÅÏö©\n            preds = torch.sigmoid(outputs).squeeze().cpu().numpy()  # Ïù¥ÏßÑ Î∂ÑÎ•òÏùò Í≤ΩÏö∞\n            preds = np.round(preds)  # Ïù¥ÏßÑ Î∂ÑÎ•òÏùò Í≤ΩÏö∞\n            predictions.extend(preds.tolist())\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:04:45.733380Z","iopub.execute_input":"2025-02-23T12:04:45.733700Z","iopub.status.idle":"2025-02-23T12:04:45.750604Z","shell.execute_reply.started":"2025-02-23T12:04:45.733673Z","shell.execute_reply":"2025-02-23T12:04:45.749711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Debugging ","metadata":{}},{"cell_type":"markdown","source":"### Debugging with `torch._dynamo.config.suppress_errors = True`\nIn deep learning model development using PyTorch, debugging runtime errors can be challenging, especially when utilizing `torch.compile()` or other optimization features. The introduction of `torch._dynamo` provides automatic graph capture and optimization for model execution. However, certain edge cases can lead to internal errors, causing execution failures. To mitigate this, `torch._dynamo.config.suppress_errors = True` is often used as a temporary debugging measure.\n\n### Understanding the Error\nWhen using `torch.compile()`, PyTorch attempts to optimize and trace the model execution graph. If an unexpected error occurs during compilation, it may lead to crashes or obscure error messages, making it difficult to identify the root cause. These errors can arise from various sources, including:\n- Unsupported Python constructs or dynamic control flows.\n- Incompatible third-party libraries.\n- Unhandled exceptions in PyTorch‚Äôs internal compilation process.\n- Graph-breaking operations that prevent optimization.\n\n### Purpose of `torch._dynamo.config.suppress_errors = True`\nSetting `torch._dynamo.config.suppress_errors = True` serves the following purposes:\n- **Error Suppression:** Instead of crashing, PyTorch will gracefully fallback to eager execution mode when an error occurs during compilation.\n- **Improved Debugging Workflow:** This allows developers to continue execution without abruptly terminating the program, helping isolate problematic code sections.\n- **Automatic Fallback Mechanism:** When an optimization fails, execution proceeds without compilation, ensuring that the model can still run.\n\n### Implications and Considerations\nWhile this setting is useful for debugging, it is important to note:\n- **Errors are hidden:** Since PyTorch suppresses internal compilation errors, developers might not be immediately aware of optimization failures.\n- **Potential Performance Degradation:** If compilation fails and the model runs in eager mode, expected speedups from `torch.compile()` will not be realized.\n- **Should Not Be Used in Production:** Suppressing errors is primarily a debugging tool and should not be enabled in a production environment where error visibility is critical.\n\n### Recommended Debugging Approach\nTo effectively debug PyTorch compilation errors:\n1. Run the model **without** `torch.compile()` to ensure it functions correctly in eager mode.\n2. Enable `torch._dynamo.config.suppress_errors = False` to observe specific error messages.\n3. If errors are still unclear, enable suppression to continue execution and isolate failing components.\n4. Use `torch._dynamo.explain(model, example_inputs)` to analyze graph capture behavior.\n5. Check for unsupported operations and try alternative model implementations if needed.\n\n### For the next training\nUsing `torch._dynamo.config.suppress_errors = True` is a valuable debugging technique when working with PyTorch‚Äôs compilation features. While it helps prevent crashes and allows execution to proceed, developers should use it cautiously and aim to resolve underlying issues instead of relying on error suppression as a long-term solution. By systematically analyzing compilation failures, models can be optimized for performance while maintaining robustness.\n\n","metadata":{}},{"cell_type":"code","source":"import torch._dynamo\ntorch._dynamo.config.suppress_errors = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:04:45.141374Z","iopub.execute_input":"2025-02-23T11:04:45.141698Z","iopub.status.idle":"2025-02-23T11:04:45.145240Z","shell.execute_reply.started":"2025-02-23T11:04:45.141658Z","shell.execute_reply":"2025-02-23T11:04:45.144382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11. Training","metadata":{}},{"cell_type":"code","source":"detector = DisasterDetector(max_seq_length=169, lr=0.00017, epochs=10, batch_size=32, patience=3)\ndetector.train(df_train)  # Î™®Îç∏ ÌïôÏäµ\nprint(\"training is completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:05:08.974514Z","iopub.execute_input":"2025-02-23T11:05:08.974847Z","iopub.status.idle":"2025-02-23T11:59:13.187195Z","shell.execute_reply.started":"2025-02-23T11:05:08.974819Z","shell.execute_reply":"2025-02-23T11:59:13.186383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12. Predicdtion","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:02:12.352540Z","iopub.execute_input":"2025-02-23T12:02:12.352838Z","iopub.status.idle":"2025-02-23T12:02:12.368390Z","shell.execute_reply.started":"2025-02-23T12:02:12.352815Z","shell.execute_reply":"2025-02-23T12:02:12.367299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ÏòàÏ∏° ÏàòÌñâ\npredictions = DisasterDetector.predict(df_test)\n\nprint(\"Predictions >>>>> \\n\", predictions))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 13. Submission","metadata":{}},{"cell_type":"code","source":"# Ï†úÏ∂ú ÌååÏùº Î°úÎìú\n\nmodel_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nprint(\"model_submission.head(): \", model_submission.head())  \nprint(model_submission.columns)  \nprint(\"Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ ÏùºÏπò Ïó¨Î∂Ä ÌôïÏù∏:   \", len(model_submission), len(y_pred))  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:06:32.129804Z","iopub.execute_input":"2025-02-23T12:06:32.130121Z","iopub.status.idle":"2025-02-23T12:06:32.144075Z","shell.execute_reply.started":"2025-02-23T12:06:32.130097Z","shell.execute_reply":"2025-02-23T12:06:32.143144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Î¶¨Ïä§Ìä∏ ÎÇ¥Î∂ÄÏùò Í∞íÎßå Ï∂îÏ∂úÌï¥ÏÑú target Ïª¨ÎüºÏóê Ìï†Îãπ\nmodel_submission[\"target\"] = model_submission[\"target\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n\n# 0 ÎòêÎäî 1Îßå ÌïÑÏöî-> int Î≥ÄÌôò\nmodel_submission[\"target\"] = model_submission[\"target\"].astype(int)\nprint(model_submission.head())  # ÌôïÏù∏\n\nmodel_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:06:35.825059Z","iopub.execute_input":"2025-02-23T12:06:35.825343Z","iopub.status.idle":"2025-02-23T12:06:35.841168Z","shell.execute_reply.started":"2025-02-23T12:06:35.825322Z","shell.execute_reply":"2025-02-23T12:06:35.840020Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission Score\n\nI conducted experiments using two different model architectures to evaluate their impact on performance.\n\n- **The first approach** : involved adding a single dropout layer followed by one linear layer.\n- **The second approach** : incorporated two dropout layers along with three fully connected (dense) layers.\n  \nAfter training and evaluating both models, the results indicated that the first approach achieved superior performance in terms of accuracy and F1 score. This suggests that a simpler architecture with fewer layers and regularization performed better, possibly due to reduced overfitting and more efficient learning. \n\nOne regret during repeating the experiment, did not save the notebool, lost the best parameter information and just got submission score result on the board. The below screen shot was from the first approach using a single dropout layer followed by one linear layer.","metadata":{}},{"cell_type":"markdown","source":"![Image](https://github.com/user-attachments/assets/72b3c6fc-ce7f-4c78-a677-9011a1359c5a)","metadata":{}},{"cell_type":"markdown","source":"# 14. Conclusion","metadata":{}},{"cell_type":"markdown","source":"- The better performance of the first experiment (with simpler setting as I mentioned initially) suggests that a simpler model was able to generalize better and avoid overfitting. The second experiment, with more complex layers, likely suffered from difficulties in training or overfitting. This highlights an important lesson: increasing model complexity doesn‚Äôt always lead to better performance. Choosing the right model complexity and regularization techniques is key to optimizing performance.\n  \n- Throughout our exploration of hyperparameter tuning, gained a deeper understanding of the critical role that learning rate adjustments play, particularly in the context of the BERT model. Our findings revealed that BERT exhibits a unique sensitivity to learning rate changes, achieving optimal performance with an exceptionally low learning rate. This insight underscores the necessity of meticulous tuning, as it can significantly influence the model's ability to adapt and perform effectively across various training phases.\n\n- In our experimentation, we also attempted to enhance the BERT architecture by adding additional layers to improve its capacity for learning complex patterns. However, despite all the efforts, the performance metrics did not meet our expectations. This prompted a reevaluation of our approach, leading us to consider RoBERTa as a promising alternative. Known for its robust training methodology and improved performance on various benchmarks, RoBERTa presents an exciting opportunity to further explore the capabilities of transformer-based models.\n\n- As we transition to RoBERTa, we remain committed to the principles of effective hyperparameter tuning, including the integration of dynamic learning rate schedules that adapt based on performance fluctuations. This approach will not only help us refine our models but also ensure that we maximize their potential in understanding and generating human-like text.\n\n- In summary, our exploration of BERT and the subsequent decision to pivot toward RoBERTa highlights the iterative nature of model development in machine learning. Each step, whether a success or a setback, contributes to our understanding and ultimately guides us toward achieving superior performance in our natural language processing endeavors. As we continue this journey, we are excited about the possibilities that lie ahead with RoBERTa and the insights we will gain through further experimentation and tuning.","metadata":{}}]}