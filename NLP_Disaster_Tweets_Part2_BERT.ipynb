{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10820614,"sourceType":"datasetVersion","datasetId":6718272},{"sourceId":10830406,"sourceType":"datasetVersion","datasetId":6725109},{"sourceId":10833186,"sourceType":"datasetVersion","datasetId":6727192}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Natural Language Processing with Disaster Tweets (Part2 -- BERT)","metadata":{}},{"cell_type":"markdown","source":"# 5. Training, fine-tuning Bert-base\n\n-----------------------------","metadata":{}},{"cell_type":"markdown","source":"# BERT-base with Classification Model\n\nIn this BERT model experiment, the first setup (with 1 dropout layer and 1 dense layer) performed better than the second setup (with 2 dropout layers and 3 dense layers). Here's how we can interpret the results from different perspectives:\n\n1. Increase in Model Complexity\nIn the second experiment, you added more dropout and dense layers (2 dropout layers and 3 dense layers). Adding more layers increases the number of parameters in the model, which can allow it to learn more complex patterns. However, this also increases the risk of overfitting. A more complex model may fit the training data too closely and fail to generalize well to unseen data. In contrast, the simpler model in the first experiment may have been better at generalizing to new data.\n\n2. Role of Dropout Layers\nDropout is a regularization technique that helps prevent overfitting by randomly ignoring certain neurons during training. However, adding too many dropout layers can make training unstable. In the first experiment, having just 1 dropout layer likely provided a good balance of regularization without causing too much information loss. In the second experiment, increasing the dropout layers to 2 might have overly constrained the model, leading to performance degradation due to too much regularization.\n\n3. Adding Dense Layers\nAdding dense layers increases the representational power of the model, but too many layers can make the learning process more challenging or cause issues like gradient vanishing. The first experiment, with only 1 dense layer, might have been sufficient to capture the important patterns in the data. In contrast, adding more layers in the second experiment might have made the model unnecessarily complex, making it harder for the model to learn effectively, and possibly leading to performance loss.\n\n4. Learning Rate and Initialization Issues\nWhen changing the model architecture, the learning rate or parameter initialization may need to be adjusted. In the second experiment, with a more complex architecture, it’s possible that the learning rate or initialization wasn’t ideal for the new setup, leading to poorer performance.\n\n\n*(One thing I regret is not properly saving the notebook information from the first experiment, so I can only submit the notebook from the second experiment.)*","metadata":{}},{"cell_type":"code","source":"!pip install transformers -q\n!pip install transformers datasets torch -q\n!pip install torch torchvision torchaudio -q\n\nprint(\"all done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T17:29:17.832086Z","iopub.execute_input":"2025-02-23T17:29:17.832362Z","iopub.status.idle":"2025-02-23T17:29:29.802445Z","shell.execute_reply.started":"2025-02-23T17:29:17.832341Z","shell.execute_reply":"2025-02-23T17:29:29.801177Z"}},"outputs":[{"name":"stdout","text":"all done\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport string\nimport operator\nimport urllib.request\nimport zipfile\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 100)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\n\nfrom transformers import BertForSequenceClassification\nfrom transformers import BertModel\nfrom transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\n\nimport torch\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom datasets import load_dataset\nfrom wordcloud import STOPWORDS\n\nimport nltk\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, GroupKFold, train_test_split, GroupShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch.nn.functional as F\nfrom datasets import load_dataset\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T17:29:29.803498Z","iopub.execute_input":"2025-02-23T17:29:29.803833Z","iopub.status.idle":"2025-02-23T17:29:53.484654Z","shell.execute_reply.started":"2025-02-23T17:29:29.803809Z","shell.execute_reply":"2025-02-23T17:29:53.483975Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed):\n    import random\n    SEED=seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T10:45:32.490149Z","iopub.execute_input":"2025-02-23T10:45:32.490452Z","iopub.status.idle":"2025-02-23T10:45:32.554872Z","shell.execute_reply.started":"2025-02-23T10:45:32.490429Z","shell.execute_reply":"2025-02-23T10:45:32.553995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check and set device (using GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:26.940891Z","iopub.execute_input":"2025-02-23T11:01:26.941341Z","iopub.status.idle":"2025-02-23T11:01:26.946337Z","shell.execute_reply.started":"2025-02-23T11:01:26.941309Z","shell.execute_reply":"2025-02-23T11:01:26.945630Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Encoding with Tokenizer","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, targets, max_len):\n        self.texts = texts\n        self.targets = targets  # ✅ targets가 None일 수도 있음\n        self.max_len =  max_len\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n            return_token_type_ids=True  # ✅ 추가됨\n        )\n        \n        # 예측 시 targets가 None일 수 있으므로 조건부로 반환\n        item = {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"token_type_ids\": encoding[\"token_type_ids\"].squeeze(0),  # ✅ 포함\n        }\n        \n        if self.targets is not None:\n            item[\"labels\"] = torch.tensor(self.targets[idx], dtype=torch.long)\n        \n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:41.717639Z","iopub.execute_input":"2025-02-23T11:01:41.718019Z","iopub.status.idle":"2025-02-23T11:01:41.724859Z","shell.execute_reply.started":"2025-02-23T11:01:41.717988Z","shell.execute_reply":"2025-02-23T11:01:41.723848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Model building on top of Bert \nImplemented a custom BertClassifier class using the BERT pre-trained layer and incorporating custom new layers (dropout layers, and dense layers), and a sigmoid activation function.\n\nFor reference, BERT-Base Uncased model (bert-en-uncased-l-12-h-768-a-12/2) has 12-layer, 768-hidden, 12-heads, 110M parameters:\n\n- 12-layer: The BERT-Base model consists of 12 transformer encoder layers. Each layer extracts high-dimensional features from the input text to contribute to context understanding.\n- 768-hidden: The hidden size (or unit) of each layer is 768. This means that each word is ultimately represented by 768 numbers, which contain the meaning and context of the word.\n- 12-heads: Each encoder layer has 12 attention heads. Multi-head attention allows the model to capture information from different aspects of the text. For example, it can focus on grammar, meaning, and structure differently.\n- 110M parameters: This model has about 110 million parameters, which indicates the amount and complexity of information learned by the model.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\nclass BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        \n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False  # BERT 가중치 업데이트 방지 (선택 사항)\n\n        self.dropout1 = nn.Dropout(0.1)\n        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)\n        self.dropout2 = nn.Dropout(0.1)\n        self.fc2 = nn.Linear(256, 32)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        clf_output = outputs.pooler_output  # [CLS]의 변환값\n        x = self.dropout1(clf_output)\n        x = self.fc1(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x  # Sigmoid 제거 (loss 함수에서 처리)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:45.566089Z","iopub.execute_input":"2025-02-23T11:01:45.566371Z","iopub.status.idle":"2025-02-23T11:01:45.572674Z","shell.execute_reply.started":"2025-02-23T11:01:45.566350Z","shell.execute_reply":"2025-02-23T11:01:45.571931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Define Metrix to Measure","metadata":{}},{"cell_type":"markdown","source":"### Explanation of Training Metrics  \n\n- **Training Precision**:  \n  This measures the proportion of **true positive predictions** out of all positive predictions made by the model. It evaluates how many of the predicted positive cases were actually correct, focusing on minimizing false positives (FP). A higher precision indicates a lower rate of incorrect positive predictions.  \n\n  \\\n  \\begin{equation}\n  \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n  \\end{equation}\n    \n\n- **Training Recall**:  \n  This metric assesses the proportion of **actual positive cases** that were correctly predicted by the model. It evaluates how well the model identifies all relevant instances while minimizing false negatives (FN). A higher recall indicates fewer missed positive cases.  \n\n  \\\n  \\begin{equation}\n  \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n  \\end{equation}\n    \n\n- **Training F1 Score**:  \n  The F1 score is the **harmonic mean** of precision and recall, balancing both metrics. It is particularly useful when dealing with imbalanced datasets, as it ensures that neither precision nor recall is disproportionately prioritized.  \n\n  \\\n  \\begin{equation}\n  \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n  \\end{equation}\n    \n\n- **Training Accuracy**:  \n  Accuracy represents the proportion of **correct predictions** (both positive and negative) out of all predictions made by the model. While useful in balanced datasets, it may not always be a reliable metric in highly imbalanced cases.  \n\n  \\\n  \\begin{equation}\n  \\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n  \\end{equation}\n    \n\n- **Training Loss**:  \n  The loss function measures the **difference between the predicted output and the actual label**. It is used to optimize the model during training, and lower values indicate better performance. Common loss functions include Cross-Entropy Loss (for classification) and Mean Squared Error (for regression).\n","metadata":{}},{"cell_type":"code","source":"class ClassificationReport:\n    def __init__(self):\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        self.train_accuracy_scores = []  # Added list for accuracy\n        self.train_loss = []\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = []\n        self.val_accuracy_scores = []  # Added list for accuracy\n        self.val_loss = []\n\n    def on_epoch_end(self, model, train_loader, val_loader, device, criterion):\n        model.eval()\n        \n        # 🔹 학습 데이터 평가\n        train_preds, train_labels, train_loss = self._predict_with_loss(model, train_loader, device, criterion)\n        train_precision = precision_score(train_labels, train_preds, average='macro')\n        train_recall = recall_score(train_labels, train_preds, average='macro')\n        train_f1 = f1_score(train_labels, train_preds, average='macro')\n        train_accuracy = np.mean(train_preds == train_labels)  # Accuracy calculation\n\n        self.train_precision_scores.append(train_precision)\n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        self.train_accuracy_scores.append(train_accuracy)  # Store accuracy\n        self.train_loss.append(train_loss)\n\n        # 🔹 검증 데이터 평가\n        val_preds, val_labels, val_loss = self._predict_with_loss(model, val_loader, device, criterion)\n        val_precision = precision_score(val_labels, val_preds, average='macro')\n        val_recall = recall_score(val_labels, val_preds, average='macro')\n        val_f1 = f1_score(val_labels, val_preds, average='macro')\n        val_accuracy = np.mean(val_preds == val_labels)  # Accuracy calculation\n\n        self.val_precision_scores.append(val_precision)\n        self.val_recall_scores.append(val_recall)\n        self.val_f1_scores.append(val_f1)\n        self.val_accuracy_scores.append(val_accuracy)  # Store accuracy\n        self.val_loss.append(val_loss)\n\n        # 🔹 Epoch별 점수 출력\n        print(f'- Training Precision: {train_precision:.6f} - Training Recall: {train_recall:.6f} - Training F1: {train_f1:.6f} - Training Accuracy: {train_accuracy:.6f} - Training Loss: {train_loss:.6f}')\n        print(f'- Validation Precision: {val_precision:.6f} - Validation Recall: {val_recall:.6f} - Validation F1: {val_f1:.6f} - Validation Accuracy: {val_accuracy:.6f} - Validation Loss: {val_loss:.6f}')\n\n        # 🔹 CUDA 메모리 정리\n        torch.cuda.empty_cache()\n\n    def _predict_with_loss(self, model, loader, device, criterion):\n        all_preds = []\n        all_labels = []\n        total_loss = 0.0\n        with torch.no_grad():\n            for batch in loader:\n                inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n                labels = batch['labels'].to(device).float().unsqueeze(1)  # (batch_size, 1)로 변환\n\n                outputs = model(**inputs)\n                loss = criterion(outputs, labels)    # 이제 labels는 float 타입이므로 오류가 발생하지 않음\n                total_loss += loss.item()\n\n                # 🔹 `sigmoid` 적용 후 `round()` 수행 (Binary Classification)\n                preds = torch.sigmoid(outputs).cpu().numpy()\n                preds = np.round(preds)  # 0 또는 1로 변환\n\n                all_preds.extend(preds)\n                all_labels.extend(labels.cpu().numpy())\n\n        return np.array(all_preds), np.array(all_labels), total_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:01:50.356808Z","iopub.execute_input":"2025-02-23T11:01:50.357210Z","iopub.status.idle":"2025-02-23T11:01:50.372184Z","shell.execute_reply.started":"2025-02-23T11:01:50.357176Z","shell.execute_reply":"2025-02-23T11:01:50.371224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Training Model","metadata":{}},{"cell_type":"markdown","source":"- For effective *hyperparameter tuning*, it is crucial to integrate $ptim.lr_scheduler$ in combination with a predefined learning rate schedule that adjusts dynamically based on patience intervals. This ensures that the model adapts to different training phases by fine-tuning the learning rate according to performance fluctuations.\n\n- Interestingly, during the fine-tuning process of the BERT model, I discovered that it achieved optimal performance when initialized with an exceptionally low learning rate. This was a rare yet significant observation, as such an approach was not commonly required for fine-tuning other models. This insight highlights the sensitivity of BERT to learning rate adjustments and underscores the importance of careful tuning to maximize its potential.","metadata":{}},{"cell_type":"code","source":"class DisasterDetector:\n    def __init__(self, max_seq_length, lr, epochs, batch_size, patience, model=None):\n        self.model = model if model is not None else BertClassifier()  # 모델 초기화\n        self.max_seq_length = max_seq_length\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.patience = patience\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.models = []\n        self.scores = {}\n        self.best_model = None  # 학습된 모델을 저장할 변수\n        self.best_model = BertClassifier().to(self.device)  # 외부에서 주어진 모델을 사용\n\n        if model:\n            self.best_model = model.to(self.device)  # 🔹 주어진 모델 사용\n\n        # 🔹 best_model이 없으면 저장된 모델 불러오기\n        if self.best_model is None and os.path.exists(\"/kaggle/input/bert-v0224/best_model.pth\"):\n            try:\n                state_dict = torch.load(\"/kaggle/input/bert-v0224/best_model.pth\", map_location=self.device)\n                self.best_model = BertClassifier().to(self.device)\n                self.best_model.load_state_dict(state_dict)\n                print(\"✅ Model loaded successfully from 'best_model.pth'\")\n\n            except Exception as e:\n                print(f\"⚠️ Model loading failed: {e}\")\n                self.best_model = None  # 모델 로드 실패 시 None으로 설정\n        else:\n            print(\"⚠️ No trained model found. Train the model first!\")\n                        \n    \n    def train(self, df_train):\n        skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n        best_accuracy = -np.inf \n        best_f1_score = -np.inf\n        best_model_state = None\n        patience_counter = 0\n        torch.cuda.empty_cache()  # ✅ 메모리 정리\n        \n        device = self.device  \n\n        ### 🔹 학습을 위한 모델은 self.best_model을 사용하지 않고, 새로운 모델을 생성\n        model = BertClassifier(freeze_bert=True).to(device)  \n        model = torch.compile(model)  # ✅ 모델 컴파일 적용 (선택 사항)\n\n        for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train['text'], df_train['target'])):\n            print(f'\\n.....[Fold {fold}].....\\n')\n            \n            train_dataset = TextDataset(df_train.loc[trn_idx, 'text'].values, df_train.loc[trn_idx, 'target'].values, max_len=self.max_seq_length)  # TextDataset(texts, labels, max_len) 형태로 데이터 전달\n            val_dataset = TextDataset(df_train.loc[val_idx, 'text'].values, df_train.loc[val_idx, 'target'].values, max_len=self.max_seq_length)\n            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True) # collate_fn=collate_fn\n            val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)  # collate_fn=collate_fn\n            print(\"Train/Val dataset are loaded...\")\n\n            optimizer = optim.Adam(model.parameters(), lr=self.lr, weight_decay=1e-5)\n            criterion = nn.BCEWithLogitsLoss()\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.3, verbose=True)\n            #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr, steps_per_epoch=len(train_loader), epochs=self.epochs)\n\n            metrics = ClassificationReport()\n            learning_rate = [0.05, 0.02, 0.105, 0.1, 0.2]  # 미리 지정한 학습률 리스트\n            lr_adjustment_count = 0  # 현재 학습률 변경 횟수\n            best_f1_score = 0\n            best_accuracy = 0\n            patience_counter = 0\n\n            for epoch in range(self.epochs):\n                print(f'Epoch: {epoch+1}')\n                model.train()\n                \n                for batch in train_loader:\n                    optimizer.zero_grad()\n                    inputs = {\"input_ids\": batch[\"input_ids\"].to(self.device), \"attention_mask\": batch[\"attention_mask\"].to(self.device), \"token_type_ids\": batch[\"token_type_ids\"].to(self.device)}  # ✅ token_type_ids 추가\n                    labels = batch['labels'].to(self.device).unsqueeze(1).float()\n                    outputs = model(**inputs)\n                    loss = criterion(outputs.view(-1, 1), labels)\n                    loss.backward()\n                    optimizer.step()\n\n                metrics.on_epoch_end(model, train_loader, val_loader, self.device, criterion)\n                val_loss = metrics.val_loss[-1]  # 마지막 검증 손실을 scalar로 저장\n                scheduler.step(val_loss)  # 스칼라 값을 전달\n                val_accuracy = metrics.val_accuracy_scores[-1]\n                val_f1 = metrics.val_f1_scores[-1]  # 불균형 데이터일 경우 F1-score를 기준으로 하는 것이 더 적절함\n\n                scheduler.step(val_loss)  # 🔹 ReduceLROnPlateau 실행\n\n                if val_f1 > best_f1_score:  # Best 모델 업데이트\n                    best_f1_score = val_f1\n                    best_model_state = model.state_dict()\n                    print(\"Best f1 score is updated!\")\n\n                if val_accuracy > best_accuracy:  # Early Stopping 기준 체크\n                    best_accuracy = val_accuracy\n                    patience_counter = 0  \n                    print(\"Best accuracy updated!\")\n                else:\n                    patience_counter += 1  \n\n                if patience_counter >= self.patience:\n                    current_lr = optimizer.param_groups[0]['lr']\n                    \n                    # 🔹 scheduler가 이미 LR을 줄여서 너무 작아졌다면, 우리가 직접 변경\n                    if current_lr < 1e-6 and lr_adjustment_count < len(learning_rate):\n                        new_lr = learning_rate[lr_adjustment_count]  \n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = new_lr  # 학습률 변경\n                            patience_counter = 0  # patience 초기화 후 다시 시도\n                            lr_adjustment_count += 1\n                            print(f\"Learning rate manually adjusted! New LR: {new_lr}, Attempts left: {len(learning_rate) - lr_adjustment_count}\")\n                    elif current_lr < 1e-6 and lr_adjustment_count >= len(learning_rate):\n                        print(\"Early stopping triggered based on accuracy & LR tuning exhausted!\")\n                        break\n        # 🔹 학습이 끝난 후, self.best_model에 가장 좋은 가중치를 로드\n        if best_model_state:\n            self.best_model = BertClassifier().to(self.device)\n            self.best_model.load_state_dict(best_model_state, strict=False)\n            torch.save(self.best_model.state_dict(), \"best_model.pth\")  # 🔹 모델 가중치 저장\n            print(\"Best model is saved in 'best_model.pth'\")\n           \n\n    def predict(self, df_test):\n        if self.best_model is None:\n            raise ValueError(\"❌ No trained model found. Train the model first!\")\n            \n    \n    self.best_model.eval()  # 모델이 None이 아니라는 게 보장됨\n    test_dataset = TextDataset(df_test['text'].values, None, max_len=self.max_seq_length)\n    test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            # 필요한 입력만 가져오기\n            inputs = {key: val.to(self.device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n            outputs = self.best_model(**inputs)\n            \n            # logits에 sigmoid 적용\n            preds = torch.sigmoid(outputs).squeeze().cpu().numpy()  # 이진 분류의 경우\n            preds = np.round(preds)  # 이진 분류의 경우\n            predictions.extend(preds.tolist())\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:04:45.733380Z","iopub.execute_input":"2025-02-23T12:04:45.733700Z","iopub.status.idle":"2025-02-23T12:04:45.750604Z","shell.execute_reply.started":"2025-02-23T12:04:45.733673Z","shell.execute_reply":"2025-02-23T12:04:45.749711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Debugging ","metadata":{}},{"cell_type":"markdown","source":"### Debugging with `torch._dynamo.config.suppress_errors = True`\nIn deep learning model development using PyTorch, debugging runtime errors can be challenging, especially when utilizing `torch.compile()` or other optimization features. The introduction of `torch._dynamo` provides automatic graph capture and optimization for model execution. However, certain edge cases can lead to internal errors, causing execution failures. To mitigate this, `torch._dynamo.config.suppress_errors = True` is often used as a temporary debugging measure.\n\n### Understanding the Error\nWhen using `torch.compile()`, PyTorch attempts to optimize and trace the model execution graph. If an unexpected error occurs during compilation, it may lead to crashes or obscure error messages, making it difficult to identify the root cause. These errors can arise from various sources, including:\n- Unsupported Python constructs or dynamic control flows.\n- Incompatible third-party libraries.\n- Unhandled exceptions in PyTorch’s internal compilation process.\n- Graph-breaking operations that prevent optimization.\n\n### Purpose of `torch._dynamo.config.suppress_errors = True`\nSetting `torch._dynamo.config.suppress_errors = True` serves the following purposes:\n- **Error Suppression:** Instead of crashing, PyTorch will gracefully fallback to eager execution mode when an error occurs during compilation.\n- **Improved Debugging Workflow:** This allows developers to continue execution without abruptly terminating the program, helping isolate problematic code sections.\n- **Automatic Fallback Mechanism:** When an optimization fails, execution proceeds without compilation, ensuring that the model can still run.\n\n### Implications and Considerations\nWhile this setting is useful for debugging, it is important to note:\n- **Errors are hidden:** Since PyTorch suppresses internal compilation errors, developers might not be immediately aware of optimization failures.\n- **Potential Performance Degradation:** If compilation fails and the model runs in eager mode, expected speedups from `torch.compile()` will not be realized.\n- **Should Not Be Used in Production:** Suppressing errors is primarily a debugging tool and should not be enabled in a production environment where error visibility is critical.\n\n### Recommended Debugging Approach\nTo effectively debug PyTorch compilation errors:\n1. Run the model **without** `torch.compile()` to ensure it functions correctly in eager mode.\n2. Enable `torch._dynamo.config.suppress_errors = False` to observe specific error messages.\n3. If errors are still unclear, enable suppression to continue execution and isolate failing components.\n4. Use `torch._dynamo.explain(model, example_inputs)` to analyze graph capture behavior.\n5. Check for unsupported operations and try alternative model implementations if needed.\n\n### For the next training\nUsing `torch._dynamo.config.suppress_errors = True` is a valuable debugging technique when working with PyTorch’s compilation features. While it helps prevent crashes and allows execution to proceed, developers should use it cautiously and aim to resolve underlying issues instead of relying on error suppression as a long-term solution. By systematically analyzing compilation failures, models can be optimized for performance while maintaining robustness.\n\n","metadata":{}},{"cell_type":"code","source":"import torch._dynamo\ntorch._dynamo.config.suppress_errors = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:04:45.141374Z","iopub.execute_input":"2025-02-23T11:04:45.141698Z","iopub.status.idle":"2025-02-23T11:04:45.145240Z","shell.execute_reply.started":"2025-02-23T11:04:45.141658Z","shell.execute_reply":"2025-02-23T11:04:45.144382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11. Training","metadata":{}},{"cell_type":"code","source":"detector = DisasterDetector(max_seq_length=169, lr=0.00017, epochs=10, batch_size=32, patience=3)\ndetector.train(df_train)  # 모델 학습\nprint(\"training is completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T11:05:08.974514Z","iopub.execute_input":"2025-02-23T11:05:08.974847Z","iopub.status.idle":"2025-02-23T11:59:13.187195Z","shell.execute_reply.started":"2025-02-23T11:05:08.974819Z","shell.execute_reply":"2025-02-23T11:59:13.186383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12. Predicdtion","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:02:12.352540Z","iopub.execute_input":"2025-02-23T12:02:12.352838Z","iopub.status.idle":"2025-02-23T12:02:12.368390Z","shell.execute_reply.started":"2025-02-23T12:02:12.352815Z","shell.execute_reply":"2025-02-23T12:02:12.367299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 예측 수행\npredictions = DisasterDetector.predict(df_test)\n\nprint(\"Predictions >>>>> \\n\", predictions))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 13. Submission","metadata":{}},{"cell_type":"code","source":"# 제출 파일 로드\n\nmodel_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nprint(\"model_submission.head(): \", model_submission.head())  \nprint(model_submission.columns)  \nprint(\"데이터 크기 일치 여부 확인:   \", len(model_submission), len(y_pred))  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:06:32.129804Z","iopub.execute_input":"2025-02-23T12:06:32.130121Z","iopub.status.idle":"2025-02-23T12:06:32.144075Z","shell.execute_reply.started":"2025-02-23T12:06:32.130097Z","shell.execute_reply":"2025-02-23T12:06:32.143144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 리스트 내부의 값만 추출해서 target 컬럼에 할당\nmodel_submission[\"target\"] = model_submission[\"target\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n\n# 0 또는 1만 필요-> int 변환\nmodel_submission[\"target\"] = model_submission[\"target\"].astype(int)\nprint(model_submission.head())  # 확인\n\nmodel_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:06:35.825059Z","iopub.execute_input":"2025-02-23T12:06:35.825343Z","iopub.status.idle":"2025-02-23T12:06:35.841168Z","shell.execute_reply.started":"2025-02-23T12:06:35.825322Z","shell.execute_reply":"2025-02-23T12:06:35.840020Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission Score\n\nI conducted experiments using two different model architectures to evaluate their impact on performance.\n\n- **The first approach** : involved adding a single dropout layer followed by one linear layer.\n- **The second approach** : incorporated two dropout layers along with three fully connected (dense) layers.\n  \nAfter training and evaluating both models, the results indicated that the first approach achieved superior performance in terms of accuracy and F1 score. This suggests that a simpler architecture with fewer layers and regularization performed better, possibly due to reduced overfitting and more efficient learning. \n\nOne regret during repeating the experiment, did not save the notebool, lost the best parameter information and just got submission score result on the board. The below screen shot was from the first approach using a single dropout layer followed by one linear layer.","metadata":{}},{"cell_type":"markdown","source":"![Image](https://github.com/user-attachments/assets/72b3c6fc-ce7f-4c78-a677-9011a1359c5a)","metadata":{}},{"cell_type":"markdown","source":"# 14. Conclusion","metadata":{}},{"cell_type":"markdown","source":"- The better performance of the first experiment (with simpler setting as I mentioned initially) suggests that a simpler model was able to generalize better and avoid overfitting. The second experiment, with more complex layers, likely suffered from difficulties in training or overfitting. This highlights an important lesson: increasing model complexity doesn’t always lead to better performance. Choosing the right model complexity and regularization techniques is key to optimizing performance.\n  \n- Throughout our exploration of hyperparameter tuning, gained a deeper understanding of the critical role that learning rate adjustments play, particularly in the context of the BERT model. Our findings revealed that BERT exhibits a unique sensitivity to learning rate changes, achieving optimal performance with an exceptionally low learning rate. This insight underscores the necessity of meticulous tuning, as it can significantly influence the model's ability to adapt and perform effectively across various training phases.\n\n- In our experimentation, we also attempted to enhance the BERT architecture by adding additional layers to improve its capacity for learning complex patterns. However, despite all the efforts, the performance metrics did not meet our expectations. This prompted a reevaluation of our approach, leading us to consider RoBERTa as a promising alternative. Known for its robust training methodology and improved performance on various benchmarks, RoBERTa presents an exciting opportunity to further explore the capabilities of transformer-based models.\n\n- As we transition to RoBERTa, we remain committed to the principles of effective hyperparameter tuning, including the integration of dynamic learning rate schedules that adapt based on performance fluctuations. This approach will not only help us refine our models but also ensure that we maximize their potential in understanding and generating human-like text.\n\n- In summary, our exploration of BERT and the subsequent decision to pivot toward RoBERTa highlights the iterative nature of model development in machine learning. Each step, whether a success or a setback, contributes to our understanding and ultimately guides us toward achieving superior performance in our natural language processing endeavors. As we continue this journey, we are excited about the possibilities that lie ahead with RoBERTa and the insights we will gain through further experimentation and tuning.","metadata":{}}]}